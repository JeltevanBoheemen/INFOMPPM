{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Recommendations based on Frequently Reviewed Together (association rules)\n",
    "For the final segment of this assignment, refer to section 5.4 of the _Practical Recommender Systems_ book (pages 113-127). After reading, download the code provided by the book and focus on the `association_rules_calculator.py` in the `builder` directory. Your task is to adapt this code for use in this notebook, translating its steps into a format suitable for our environment. Here's a simplified outline based on the source code:\n",
    "\n",
    "The steps found in the source code are:\n",
    "1. Load the data\n",
    "2. Generate transactions or, in our case reviews\n",
    "3. Calculate the Support Confidence\n",
    "4. Save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "Instead of using a database, load your `.csv` files into a dataframe. Select the data necessary for identifying which user reviewed which books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/ratings_subset.csv', sep=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating the reviews\n",
    "In this context, transactions are the reviews. You need to compile a list of lists, where each inner list contains reviews that are related, similar to how shopping lists are grouped in the example: `[['eggs','milk','bread'], ['bacon', 'bread'], [...], [...]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_reviews = df.groupby(['User-ID'])\n",
    "reviews_list = grouped_reviews['ISBN'].apply(list)\n",
    "\n",
    "reviews = [rs for rs in reviews_list]\n",
    "# or reviews.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate the Support Confidence\n",
    "This requires some puzzling, but looking at the source code will give you a clear idea. You can reuse the subroutines in the source code and pass along the list containing the reviews belonging together. Play around with the _minimum support_ parameter. Too strict will result in fewer associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted code from book\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_association_rules(one_itemsets, two_itemsets, N):\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    rules = []\n",
    "    for source, source_freq in one_itemsets.items():\n",
    "        for key, group_freq in two_itemsets.items():\n",
    "            if source.issubset(key):\n",
    "                target = key.difference(source)\n",
    "                support = group_freq / N\n",
    "                confidence = group_freq / source_freq\n",
    "                rules.append((timestamp, next(iter(source)), next(iter(target)),\n",
    "                              confidence, support))\n",
    "    return rules\n",
    "\n",
    "def calculate_itemsets_one(transactions, min_sup=0.01):\n",
    "\n",
    "    N = len(transactions)\n",
    "\n",
    "    temp = defaultdict(int)\n",
    "    one_itemsets = dict()\n",
    "\n",
    "    for items in transactions:\n",
    "        for item in items:\n",
    "            inx = frozenset({item})\n",
    "            temp[inx] += 1\n",
    "\n",
    "    # remove all items that is not supported.\n",
    "    for key, itemset in temp.items():\n",
    "        if itemset > min_sup * N:\n",
    "            one_itemsets[key] = itemset\n",
    "\n",
    "    return one_itemsets\n",
    "\n",
    "def calculate_itemsets_two(transactions, one_itemsets):\n",
    "    two_itemsets = defaultdict(int)\n",
    "\n",
    "    for items in transactions:\n",
    "        items = list(set(items))  # remove duplications\n",
    "\n",
    "        if (len(items) > 2):\n",
    "            for perm in combinations(items, 2):\n",
    "                if has_support(perm, one_itemsets):\n",
    "                    two_itemsets[frozenset(perm)] += 1\n",
    "        elif len(items) == 2:\n",
    "            if has_support(items, one_itemsets):\n",
    "                two_itemsets[frozenset(items)] += 1\n",
    "    return two_itemsets\n",
    "\n",
    "def calculate_association_rules(one_itemsets, two_itemsets, N):\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    rules = []\n",
    "    for source, source_freq in one_itemsets.items():\n",
    "        for key, group_freq in two_itemsets.items():\n",
    "            if source.issubset(key):\n",
    "                target = key.difference(source)\n",
    "                support = group_freq / N\n",
    "                confidence = group_freq / source_freq\n",
    "                rules.append((timestamp, next(iter(source)), next(iter(target)),\n",
    "                              confidence, support))\n",
    "    return rules\n",
    "\n",
    "def has_support(perm, one_itemsets):\n",
    "    return frozenset({perm[0]}) in one_itemsets and \\\n",
    "           frozenset({perm[1]}) in one_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_association_rules(reviews, min_support=0.01):\n",
    "    n = len(reviews)\n",
    "    one_itemsets = calculate_itemsets_one(reviews, min_support)\n",
    "    two_itemsets = calculate_itemsets_two(reviews, one_itemsets)\n",
    "    rules = calculate_association_rules(one_itemsets, two_itemsets, n)\n",
    "    return rules\n",
    "\n",
    "rules = build_association_rules(reviews, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save the results\n",
    "Create a dataframe for the results of step 3. In order to make it work with the current app please make sure the columns are `source;target;support;confidence`. Save the recommendations as `recommendations-seeded-associations.csv` and replace the file in the app directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df = pd.DataFrame(rules, columns=['timestamp','source', 'target', 'support', 'confidence'])\n",
    "\n",
    "# drop timestamp\n",
    "rules_df = rules_df.drop('timestamp', axis='columns')\n",
    "\n",
    "# save the dataframe\n",
    "rules_df.to_csv('app/recommendations/recommendations-seeded-associations.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c10f95d263926787ebf1d430d11186fc6b9bac835b8518e0b5006ed24f0c36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
